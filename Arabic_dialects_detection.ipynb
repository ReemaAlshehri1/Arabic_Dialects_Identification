{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WBK81EaKGhU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUBSYIviDkKz"
      },
      "outputs": [],
      "source": [
        "# read the datasets\n",
        "data1 = pd.read_csv('/content/ArSarcasm.csv')\n",
        "\n",
        "data2 = pd.read_csv('/content/DL_cleaned_train.csv')\n",
        "\n",
        "data3 = pd.read_csv('/content/Arabic_dialect.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejzJB9XLDkIM"
      },
      "outputs": [],
      "source": [
        "# craete dataframes of datasets\n",
        "df1=pd.DataFrame(data1)\n",
        "df2=pd.DataFrame(data2)\n",
        "df3=pd.DataFrame(data3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9nkvLar0Dvz"
      },
      "source": [
        "# 1- get insights of data\n",
        "## -get info of data\n",
        "## -count the null values\n",
        "## -count the number of labels in each dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brXUynTDDkF2"
      },
      "outputs": [],
      "source": [
        "df1.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8vPyocIEfOS"
      },
      "outputs": [],
      "source": [
        "df2.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-wPUpktLxC5"
      },
      "outputs": [],
      "source": [
        "df3.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXfC7y9PDkDP"
      },
      "outputs": [],
      "source": [
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TISf1jTYEjkI"
      },
      "outputs": [],
      "source": [
        "df2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEFV81N-L0V0"
      },
      "outputs": [],
      "source": [
        "df3.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz2zMpWAFGQr"
      },
      "outputs": [],
      "source": [
        "df1['dialect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZGfeaMyEmFE"
      },
      "outputs": [],
      "source": [
        "df2['Dialect'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amg2KA18L3ji"
      },
      "outputs": [],
      "source": [
        "df3['result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK-mvqkO_mVa"
      },
      "outputs": [],
      "source": [
        "df1=df1[df1['dialect'].isin(['msa','gulf','levant'])] ## we don't need the egypt and magreb classes  bec we have enough in df2\n",
        "df3 = df3[df3['result'].isin(['G'])] # we need just the gulf class of df3 to balance the number of instances in  ll labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXnDrymb97YE"
      },
      "source": [
        "# 2- preprocessing data\n",
        "## -drop missing values\n",
        "## -lowercasing labels names in all dataframes\n",
        "## -convert labels to numbers\n",
        "## -in each dataframe keep text and dialect ids coulmns\n",
        "## -combine the 3 dataframes\n",
        "## -cleanning the text coulmn\n",
        "## -delete short text\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqnM1Cv593n9"
      },
      "outputs": [],
      "source": [
        "# drop the messing valuses in df3\n",
        "df3=df3.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENwWE3EWJTu0"
      },
      "outputs": [],
      "source": [
        "# converting all dialect (labels) in the three DataFrames to lowercase letters\n",
        "df1['dialect'] = df1['dialect'].str.lower()\n",
        "df2['Dialect'] = df2['Dialect'].str.lower()\n",
        "df3['result'] = df3['result'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wna-jrvFIxK3"
      },
      "outputs": [],
      "source": [
        "# map each label to a number\n",
        "dialect_map = {\n",
        "    'msa': 0,\n",
        "    'eg': 1,\n",
        "    'gulf': 2, 'g': 2,\n",
        "    'levant': 3, 'lb': 3,\n",
        "    'magreb': 4, 'ma': 4, 'ly': 4,\n",
        "    'sd': 5  #\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8fz0lpYJcn0"
      },
      "outputs": [],
      "source": [
        "# convert the dilect classes to numbers(ids) using the previous map\n",
        "df1['dialect_id'] = df1['dialect'].map(dialect_map)\n",
        "df2['dialect_id'] = df2['Dialect'].map(dialect_map)\n",
        "df3['dialect_id'] = df3['result'].map(dialect_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkNzaAxQJqu5"
      },
      "outputs": [],
      "source": [
        "# only drop rows where dialect_id is missing\n",
        "df1 = df1.dropna(subset=['dialect_id'])\n",
        "df2 = df2.dropna(subset=['dialect_id'])\n",
        "df3 = df3.dropna(subset=['dialect_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2H8xf-wJtv5"
      },
      "outputs": [],
      "source": [
        "# in each dataframe keep only the text and dialect_id\n",
        "df1_final = df1[['tweet', 'dialect_id']]\n",
        "df2_final = df2[['Text', 'dialect_id']]\n",
        "df3_final = df3[['Tweet', 'dialect_id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6FPRZFyKFbN"
      },
      "outputs": [],
      "source": [
        "# Rename 'tweet' column in df1 and df3 to 'Text'\n",
        "df1_final = df1_final.rename(columns={'tweet': 'Text'})\n",
        "df3_final = df3_final.rename(columns={'Tweet': 'Text'})\n",
        "\n",
        "# Combine DataFrames into single dataframe\n",
        "combined_df = pd.concat([df1_final, df2_final,df3_final], ignore_index=True)\n",
        "\n",
        "# check result\n",
        "print(combined_df.head())\n",
        "print(combined_df['dialect_id'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJU5x3qb6jp5"
      },
      "outputs": [],
      "source": [
        "# change name to final_df\n",
        "final_df = combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqPc_L_eV8VY"
      },
      "outputs": [],
      "source": [
        "# install necessary libraries\n",
        "!pip install farasapy\n",
        "!pip install arabert\n",
        "!pip install transformers datasets peft accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILLsTMDSV8SC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# function to clean the the Text column\n",
        "def manual_clean(text):\n",
        "  text=str(text)\n",
        "  text=re.sub(r'@[\\w_]+', ' ', text)\n",
        "  text=re.sub(r'http\\S+', ' ', text)\n",
        "  text = re.sub(r'#\\S+', '', text)       # remove hashtags\n",
        "  text = re.sub(r'[a-zA-Z]+', ' ', text)# remove English letters\n",
        "  text = re.sub(r'\\d+', ' ', text)       # remove digits\n",
        "  text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)  # keep Arabic only\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5mMhew16Roi"
      },
      "outputs": [],
      "source": [
        "# apply the previous function to the Text column\n",
        "final_df['Text'] = final_df['Text'].apply(manual_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GFkM3ST7TYZ"
      },
      "outputs": [],
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "# create a preprocessor instance specific to AraBERTv2\n",
        "arabert_prep = ArabertPreprocessor(model_name=\"aubmindlab/bert-base-arabertv2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIrKRvWt6Txw"
      },
      "outputs": [],
      "source": [
        "# apply the arabert preprocessing to the Text column\n",
        "final_df['Text'] = final_df['Text'].apply(arabert_prep.preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnp5OISB7jWt"
      },
      "outputs": [],
      "source": [
        "# removing short tweets\n",
        "final_df = final_df[final_df['Text'].str.len() > 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tinuIM0-0zYC"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# convert a Pandas DataFrame (final_df) into a Hugging Face Dataset\n",
        "# you should have a column named \"text\" and \"label\" (numeric label) to match Hugging Face Trainer expectations\n",
        "dataset = Dataset.from_pandas(final_df[['Text', 'dialect_id']].rename(columns={\n",
        "    'Text': 'text',\n",
        "    'dialect_id': 'label'\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ML6YS8r--Y6"
      },
      "source": [
        "# 3- Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cda2mg1u0zT4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Specify the name of the pretrained AraBERT v2 model from Hugging Face Hub\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "# Load the tokenizer associated with the AraBERT v2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load the pre-trained BERT model and add a classification head with 6 output labels\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7I7vBu51ccW"
      },
      "outputs": [],
      "source": [
        "# tokeniziation\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnZkSZtK1cZf"
      },
      "outputs": [],
      "source": [
        "# split dataset to train and eval sets\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "eval_dataset = tokenized_dataset[\"test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w49F6NXY1cWZ"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "#prepares efficient fine-tuning of the AraBERT model using LoRA adapters\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "# inject LoRA adapters into the base model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "# print which parameters will be trained\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mQpvm0F2DeE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# define a compute metrices function\n",
        "def compute_metrics(pred):\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpHcfTrvEiJs"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# disable wandb logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "# compute class weights from training labels\n",
        "labels = train_dataset[\"label\"]\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# patch the compute_loss to include class weights\n",
        "def custom_compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tensor.to(logits.device))\n",
        "    loss = loss_fn(logits, labels)\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# patch the Trainer class\n",
        "Trainer.compute_loss = custom_compute_loss\n",
        "\n",
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./arabert-dialect-lora\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# padding collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zahEOLGE2Y8W"
      },
      "outputs": [],
      "source": [
        "# save the model and the tokenizer\n",
        "model.save_pretrained(\"./arabert-dialect-lora\")\n",
        "tokenizer.save_pretrained(\"./arabert-dialect-lora\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rzh-YkTXERw"
      },
      "outputs": [],
      "source": [
        "#zipping the model\n",
        "!zip -r arabert-dialect-lora.zip arabert-dialect-lora\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Wwl0yvC0vm"
      },
      "source": [
        "# 4- Loading the model and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrhz_N2h7xTq"
      },
      "outputs": [],
      "source": [
        "# load the fine-tuned model and unzipp it\n",
        "!unzip /content/arabert-dialect-lora.zip -d arabert-dialect-lora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th9cBbZGrR2n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_path = \"/content/arabert-dialect-lora\"\n",
        "\n",
        "# load LoRA config\n",
        "config = PeftConfig.from_pretrained(peft_model_path, local_files_only=True)\n",
        "\n",
        "# load base model\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    num_labels=6\n",
        ")\n",
        "\n",
        "# load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_path, local_files_only=True)\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path, local_files_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1IBwzp1rgpA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# create a trainer for evaluation\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "metrics = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CF6JAXWyBmB"
      },
      "outputs": [],
      "source": [
        "# print the accuarcy of the model\n",
        "print(metrics[\"eval_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvnd3UGFDTTh"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
